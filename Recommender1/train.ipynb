{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "32406f7d",
   "metadata": {},
   "source": [
    "# Custom \"Next Tool Predictor\" with Confidence Score"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3910569c",
   "metadata": {},
   "source": [
    "### ðŸ’¡ Import Data and Libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "d1b73038",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\vaibh\\anaconda3\\envs\\llm\\Lib\\site-packages\\sentence_transformers\\cross_encoder\\CrossEncoder.py:11: TqdmExperimentalWarning: Using `tqdm.autonotebook.tqdm` in notebook mode. Use `tqdm.tqdm` instead to force console mode (e.g. in jupyter console)\n",
      "  from tqdm.autonotebook import tqdm, trange\n",
      "c:\\Users\\vaibh\\anaconda3\\envs\\llm\\Lib\\site-packages\\threadpoolctl.py:1214: RuntimeWarning: \n",
      "Found Intel OpenMP ('libiomp') and LLVM OpenMP ('libomp') loaded at\n",
      "the same time. Both libraries are known to be incompatible and this\n",
      "can cause random crashes or deadlocks on Linux when loaded in the\n",
      "same Python program.\n",
      "Using threadpoolctl may cause crashes or deadlocks. For more\n",
      "information and possible workarounds, please see\n",
      "    https://github.com/joblib/threadpoolctl/blob/master/multiple_openmp.md\n",
      "\n",
      "  warnings.warn(msg, RuntimeWarning)\n"
     ]
    }
   ],
   "source": [
    "# Import all libraries\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "import random\n",
    "import numpy as np\n",
    "from sentence_transformers import SentenceTransformer\n",
    "from collections import defaultdict, Counter\n",
    "import torch.nn.functional as F\n",
    "\n",
    "# Import data\n",
    "from data import tool_vocab, tool_descriptions, tool_patterns, common_workflows, test_histories"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6fa1065e",
   "metadata": {},
   "source": [
    "### ðŸ’¡Set Configuration\n",
    " - Set device to \"cuda\" if cuda is available or default to \"cpu\"\n",
    " - Set pad_token_id to 0. Padding token is used as indice in place of padding "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aab53f7a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using device: cuda\n"
     ]
    }
   ],
   "source": [
    "# Configuration Setup\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\") \n",
    "print(f\"Using device: {device}\")\n",
    "\n",
    "# Set padding token id\n",
    "pad_token_id = 0\n",
    "\n",
    "# Mappings\n",
    "tool_to_id = {tool: idx + 1 for idx, tool in enumerate(tool_vocab)}\n",
    "id_to_tool = {idx + 1: tool for idx, tool in enumerate(tool_vocab)}\n",
    "\n",
    "# Adding +1 to tool_vocab to account for padding token\n",
    "vocab_size = len(tool_vocab) + 1\n",
    "\n",
    "# Setting context length to 6 for this demo\n",
    "context_len = 6 "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "281de03d",
   "metadata": {},
   "source": [
    "### ðŸ’¡Generate Embeddings\n",
    "- Using pretrained embedding model via sentence transformer for description embedding generation "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "13360fad",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Shape of embeddings: (55, 384)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\vaibh\\anaconda3\\envs\\llm\\Lib\\site-packages\\transformers\\models\\bert\\modeling_bert.py:408: UserWarning: 1Torch was not compiled with flash attention. (Triggered internally at C:\\actions-runner\\_work\\pytorch\\pytorch\\builder\\windows\\pytorch\\aten\\src\\ATen\\native\\transformers\\cuda\\sdp_utils.cpp:555.)\n",
      "  attn_output = torch.nn.functional.scaled_dot_product_attention(\n"
     ]
    }
   ],
   "source": [
    "# Generate Description Embeddings\n",
    "'''\n",
    "Using sentence transformers\n",
    "'''\n",
    "# Using pre trained embedding model for embedding generation\n",
    "desc_model = SentenceTransformer('all-MiniLM-L6-v2')\n",
    "desc_texts = [tool_descriptions[tool] for tool in tool_vocab]\n",
    "desc_embeddings = desc_model.encode(desc_texts, normalize_embeddings=True)\n",
    "desc_id_to_embedding = {\n",
    "    tool_to_id[tool]: torch.tensor(desc_embeddings[i], dtype=torch.float32).to(device)\n",
    "    for i, tool in enumerate(tool_vocab)\n",
    "}\n",
    "\n",
    "# Print size of generated embeddings \n",
    "print(f\"Shape of embeddings: {desc_embeddings.shape}\") \n",
    "desc_dim = desc_embeddings.shape[1]\n",
    "\n",
    "# Add padding token embedding\n",
    "desc_id_to_embedding[pad_token_id] = torch.zeros(desc_dim, dtype=torch.float32).to(device)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "677ce6d0",
   "metadata": {},
   "source": [
    "### ðŸ’¡Data Preparation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "75419983",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Data Preparation\n",
    "class ToolDataset(Dataset):\n",
    "    def __init__(self, sequences, context_len, augment_data=True):\n",
    "        self.samples = []\n",
    "        self.pattern_freq = defaultdict(int)\n",
    "        self.context_len = context_len\n",
    "        \n",
    "        # First pass: collect pattern frequencies\n",
    "        for seq in sequences:\n",
    "            token_ids = [tool_to_id[t] for t in seq]\n",
    "            for i in range(1, len(token_ids)):\n",
    "                context = token_ids[max(0, i - context_len):i]\n",
    "                label = token_ids[i]\n",
    "                \n",
    "                # Using last 3 tokens as patterns\n",
    "                pattern = tuple(context[-min(3, len(context)):])\n",
    "                self.pattern_freq[pattern] += 1\n",
    "        \n",
    "        # Second pass: create samples with weights\n",
    "        for seq in sequences:\n",
    "            token_ids = [tool_to_id[t] for t in seq]\n",
    "            for i in range(1, len(token_ids)):\n",
    "                context = token_ids[max(0, i - context_len):i]\n",
    "                label = token_ids[i]\n",
    "                context = [pad_token_id] * (context_len - len(context)) + context\n",
    "                \n",
    "                # Calculate sample weight based on pattern frequency\n",
    "                pattern = tuple(context[-min(3, len(context)):])\n",
    "                # Reduce weight for common patterns\n",
    "                weight = 1.0 / (1 + self.pattern_freq[pattern] * 0.1)  \n",
    "                \n",
    "                self.samples.append((context, label, weight))\n",
    "    \n",
    "    def __len__(self):\n",
    "        # Returns length of samples \n",
    "        return len(self.samples)\n",
    "    \n",
    "    def __getitem__(self, idx):\n",
    "        # Returns data present at an index\n",
    "        context, label, weight = self.samples[idx]\n",
    "        return (torch.tensor(context, dtype=torch.long), \n",
    "                torch.tensor(label, dtype=torch.long),\n",
    "                torch.tensor(weight, dtype=torch.float32))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "92fe6e7a",
   "metadata": {},
   "source": [
    "### ðŸ’¡Custom Model Architecture\n",
    "\n",
    "ðŸ“Œ Key Components & Design Highlights\n",
    "**Input Embedding Layers:**\n",
    "\n",
    "**1. Token Embedding**: Learns dense representations for input tokens.\n",
    "\n",
    "**2. Positional Embedding**: Encodes positional information to preserve token order in sequences.\n",
    "\n",
    "**3. Description Embedding**: Projects external, pre-computed description vectors (desc_emb_table) into the model's embedding space using a linear layer (desc_proj).\n",
    "\n",
    "**Embedding Fusion & Normalization:**\n",
    "\n",
    "- Combines token, positional, and description embeddings for each token.\n",
    "\n",
    "- Apply LayerNorm and dropout for stabilization and regularization.\n",
    "\n",
    "**Transformer Decoder Layers:**\n",
    "\n",
    "- A stack of enhanced nn.TransformerDecoderLayer modules with residual connections.\n",
    "\n",
    "- Each layer processes the input using masked self-attention to prevent future token leakage.\n",
    "\n",
    "**Contextual Attention Mechanism:**\n",
    "\n",
    "- A separate MultiheadAttention layer re-weights token-level hidden states to capture contextual importance.\n",
    "\n",
    "- Output is added back to the hidden representation via a residual connection.\n",
    "\n",
    "**Multiple Output Heads:**\n",
    "\n",
    "- Logits Head: Generates token predictions using a linear projection.\n",
    "\n",
    "- Confidence Head: Outputs a scalar confidence score (0 to 1) using a sigmoid activation.\n",
    "\n",
    "**Weight Initialization:**\n",
    "\n",
    "- Linear layers use Xavier uniform initialization for stability.\n",
    "\n",
    "- Embeddings are initialized with a normal distribution."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1815143d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Custom Model Architecture \n",
    "class ToolPredictor(nn.Module):\n",
    "    def __init__(self, vocab_size, embed_dim, desc_dim, n_heads, num_layers, context_len, desc_emb_table, dropout=0.1):\n",
    "        super().__init__()\n",
    "        self.embed_dim = embed_dim\n",
    "        self.context_len = context_len\n",
    "        self.desc_dim = desc_dim\n",
    "        \n",
    "        # Enhanced embeddings\n",
    "        self.token_embedding = nn.Embedding(vocab_size, embed_dim, padding_idx=pad_token_id)\n",
    "        self.pos_embedding = nn.Embedding(context_len, embed_dim)\n",
    "        self.desc_proj = nn.Linear(desc_dim, embed_dim)\n",
    "        self.desc_emb_table = desc_emb_table\n",
    "        \n",
    "        # Layer normalization for embeddings\n",
    "        self.embed_norm = nn.LayerNorm(embed_dim)\n",
    "        \n",
    "        # Enhanced transformer with residual connections\n",
    "        self.transformer_layers = nn.ModuleList([\n",
    "            nn.TransformerDecoderLayer(\n",
    "                d_model=embed_dim, \n",
    "                nhead=n_heads, \n",
    "                dropout=dropout,\n",
    "                batch_first=True\n",
    "            )\n",
    "            for _ in range(num_layers)\n",
    "        ])\n",
    "        \n",
    "        # Attention mechanism for context weighting\n",
    "        self.context_attention = nn.MultiheadAttention(embed_dim, n_heads, dropout=dropout, batch_first=True)\n",
    "        \n",
    "        # Multiple prediction heads\n",
    "        self.output_layer = nn.Linear(embed_dim, vocab_size)\n",
    "\n",
    "        # Confidence scoring\n",
    "        self.confidence_head = nn.Linear(embed_dim, 1)  \n",
    "        \n",
    "        # Dropout for regularization\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "        \n",
    "        # Initialize weights\n",
    "        self._init_weights()\n",
    "    \n",
    "    def _init_weights(self):\n",
    "        # self.modules() is a built-in PyTorch method which finds every submodule in model\n",
    "        for module in self.modules():\n",
    "            if isinstance(module, nn.Linear):\n",
    "                # If a module is a Linear layer, its weights are initialized using Xavier Uniform\n",
    "                nn.init.xavier_uniform_(module.weight)\n",
    "                if module.bias is not None:\n",
    "                    nn.init.zeros_(module.bias)\n",
    "\n",
    "            elif isinstance(module, nn.Embedding):\n",
    "                # If a module is an Embedding layer, its weights are initialized from a normal distribution\n",
    "                nn.init.normal_(module.weight, std=0.02)\n",
    "    \n",
    "    def forward(self, x):\n",
    "        batch_size, seq_len = x.size()\n",
    "        \n",
    "        # Token embeddings\n",
    "        # Output size: (batch_size, seq_len, embed_dim)\n",
    "        tok_emb = self.token_embedding(x)\n",
    "        \n",
    "        # Positional embeddings\n",
    "        # Output size: (batch_size, seq_len, embed_dim)\n",
    "        pos_ids = torch.arange(seq_len, device=x.device).unsqueeze(0).expand(batch_size, -1)\n",
    "        pos_emb = self.pos_embedding(pos_ids)\n",
    "        \n",
    "        # Description embeddings\n",
    "        # Output size: (batch_size, seq_len, embed_dim)\n",
    "        desc_emb = torch.stack([\n",
    "            torch.stack([\n",
    "                self.desc_proj(self.desc_emb_table.get(tok.item(), torch.zeros(self.desc_dim).to(x.device)))\n",
    "                for tok in row\n",
    "            ])\n",
    "            for row in x\n",
    "        ])\n",
    "        \n",
    "        # Combine embeddings\n",
    "        # Output size: (batch_size, seq_len, embed_dim)\n",
    "        x_emb = tok_emb + pos_emb + desc_emb\n",
    "        x_emb = self.embed_norm(x_emb)\n",
    "        x_emb = self.dropout(x_emb)\n",
    "        \n",
    "        # Create causal mask\n",
    "        tgt_mask = nn.Transformer.generate_square_subsequent_mask(seq_len).to(x.device)\n",
    "        \n",
    "        # Pass through transformer layers\n",
    "        # Output size: (batch_size, seq_len, embed_dim)\n",
    "        hidden = x_emb\n",
    "        for layer in self.transformer_layers:\n",
    "            hidden = layer(hidden, hidden, tgt_mask=tgt_mask)\n",
    "        \n",
    "        # Context attention for final representation\n",
    "        # Output size: (batch_size, seq_len, embed_dim)\n",
    "        attn_out, _ = self.context_attention(hidden, hidden, hidden)\n",
    "\n",
    "        # Residual connection (Skip Connection)\n",
    "        # Output size: (batch_size, seq_len, embed_dim)\n",
    "        final_hidden = hidden + attn_out  \n",
    "        \n",
    "        # Use last token for prediction \n",
    "        # Output Size: (batch_size, embed_dim)\n",
    "        last_hidden = final_hidden[:, -1, :]\n",
    "        \n",
    "        # Output Raw Logits\n",
    "        # Output size: (batch_size, vocab_size)\n",
    "        logits = self.output_layer(last_hidden)\n",
    "\n",
    "        # Prediction Confidence\n",
    "        # Output size: (batch_size, 1)\n",
    "        confidence = torch.sigmoid(self.confidence_head(last_hidden)) \n",
    "        \n",
    "        return logits, confidence"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "95878c2e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Focal Loss\n",
    "class FocalLoss(nn.Module):\n",
    "    def __init__(self, alpha=1, gamma=2, reduction='mean'):\n",
    "        super().__init__()\n",
    "        self.alpha = alpha\n",
    "        self.gamma = gamma\n",
    "        self.reduction = reduction\n",
    "    \n",
    "    def forward(self, inputs, targets):\n",
    "        ce_loss = F.cross_entropy(inputs, targets, reduction='none')\n",
    "        pt = torch.exp(-ce_loss)\n",
    "        focal_loss = self.alpha * (1-pt)**self.gamma * ce_loss\n",
    "        \n",
    "        if self.reduction == 'mean':\n",
    "            return focal_loss.mean()\n",
    "        elif self.reduction == 'sum':\n",
    "            return focal_loss.sum()\n",
    "        else:\n",
    "            return focal_loss"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "313d694a",
   "metadata": {},
   "source": [
    "### ðŸ’¡Training Loop\n",
    "\n",
    "ðŸ“Œ **Key Components & Techniques**\n",
    "1. **Optimizer and Learning Rate Scheduling**\n",
    "- Optimizer: Uses AdamW, a variation of Adam with weight decay, which improves generalization\n",
    "- Scheduler: ReduceLROnPlateau reduces the learning rate when validation performance (loss) stagnates, helping fine-tune learning in later epochs\n",
    "\n",
    "2. **Loss Functions**\n",
    "- Focal Loss: Applied to main token prediction to focus learning on hard examples and handle class imbalance more effectively\n",
    "- Binary Cross Entropy (BCE) Loss: Used for confidence prediction, encouraging the model to assign high confidence to correct predictions and lower confidence otherwise.\n",
    "\n",
    "3. **Training Loop**\n",
    "- The model is set to training mode (model.train()).\n",
    "For each batch:\n",
    "Inputs, labels, and sample weights are moved to the target device.\n",
    "Forward pass yields both class logits and prediction confidence.\n",
    "Two losses are computed:\n",
    "\n",
    "- pred_loss: Main task loss from FocalLoss\n",
    "- conf_loss: Confidence score loss using BCE against correctness of predictions\n",
    "\n",
    "The total loss is a combination:\n",
    "- weighted_loss = (pred_loss + 0.1 * conf_loss) * sample_weights\n",
    "\n",
    "Backpropagation and optimization are performed with:\n",
    "\n",
    "- Gradient clipping (max norm = 1.0) to prevent exploding gradients\n",
    "- Zeroing gradients and calling .backward() and .step()\n",
    "\n",
    "\n",
    "4. **Model Checkpointing**\n",
    "- The model is saved whenever a new lowest average loss is achieved.\n",
    "Saved checkpoint includes:\n",
    "- Epoch number\n",
    "- Model state\n",
    "- Optimizer state\n",
    "- Best loss\n",
    "\n",
    "5. **Logging & Monitoring**\n",
    "- Logs training loss and learning rate every 5 epochs.\n",
    "\n",
    "Scheduler dynamically adjusts the learning rate based on average loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ceab2020",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Setup training loop\n",
    "def train_model(model, dataloader, epochs=100, lr=1e-3, save_path=\"best_model.pth\"):\n",
    "    optimizer = torch.optim.AdamW(model.parameters(), lr=lr, weight_decay=0.01)\n",
    "    scheduler = torch.optim.lr_scheduler.ReduceLROnPlateau(optimizer, patience=5, factor=0.5)\n",
    "    \n",
    "    # Use focal loss for better handling of class imbalance\n",
    "    loss_fn = FocalLoss(alpha=1, gamma=2)\n",
    "    confidence_loss_fn = nn.BCELoss()\n",
    "    \n",
    "    model.train()\n",
    "    best_loss = float('inf')\n",
    "    \n",
    "    for epoch in range(epochs):\n",
    "        total_loss = 0\n",
    "        total_samples = 0\n",
    "        \n",
    "        for batch_data in dataloader:\n",
    "            context, label, weights = batch_data\n",
    "            context, label, weights = context.to(device), label.to(device), weights.to(device)\n",
    "            \n",
    "            logits, confidence = model(context)\n",
    "            \n",
    "            # Main prediction loss\n",
    "            pred_loss = loss_fn(logits, label)\n",
    "            \n",
    "            # Confidence loss (high confidence for correct predictions)\n",
    "            pred_correct = (torch.argmax(logits, dim=-1) == label).float()\n",
    "            conf_loss = confidence_loss_fn(confidence.squeeze(), pred_correct)\n",
    "            \n",
    "            # Weighted total loss\n",
    "            total_loss_batch = pred_loss + 0.1 * conf_loss\n",
    "            weighted_loss = (total_loss_batch * weights).mean()\n",
    "            \n",
    "            optimizer.zero_grad()\n",
    "            weighted_loss.backward()\n",
    "            torch.nn.utils.clip_grad_norm_(model.parameters(), max_norm=1.0)  # Gradient clipping\n",
    "            optimizer.step()\n",
    "            \n",
    "            total_loss += weighted_loss.item()\n",
    "            total_samples += len(context)\n",
    "        \n",
    "        avg_loss = total_loss / len(dataloader)\n",
    "        scheduler.step(avg_loss)\n",
    "        \n",
    "        if avg_loss < best_loss:\n",
    "            best_loss = avg_loss\n",
    "            print(f\"Epoch {epoch+1}/{epochs}: New best model found with loss: {avg_loss:.4f}. Saving checkpoint...\")\n",
    "            torch.save({\n",
    "                'epoch': epoch,\n",
    "                'model_state_dict': model.state_dict(),\n",
    "                'optimizer_state_dict': optimizer.state_dict(),\n",
    "                'loss': best_loss,\n",
    "            }, save_path)\n",
    "        \n",
    "        if epoch % 5 == 0:\n",
    "            print(f\"Epoch {epoch+1}/{epochs}, Loss: {avg_loss:.4f}, LR: {optimizer.param_groups[0]['lr']:.6f}\")\n",
    "    \n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "474d3b1d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ==== Enhanced Inference with Confidence ====\n",
    "def predict_next_with_confidence(model, history, top_k=3):\n",
    "    model.eval()\n",
    "    with torch.no_grad():\n",
    "        context = [tool_to_id[t] for t in history][-context_len:]\n",
    "        context = [pad_token_id] * (context_len - len(context)) + context\n",
    "        context = torch.tensor([context], dtype=torch.long).to(device)\n",
    "        \n",
    "        logits, confidence = model(context)\n",
    "        probabilities = F.softmax(logits, dim=-1)\n",
    "        \n",
    "        # Get top-k predictions\n",
    "        top_probs, top_indices = torch.topk(probabilities, top_k, dim=-1)\n",
    "        \n",
    "        predictions = []\n",
    "        for i in range(top_k):\n",
    "            tool_id = top_indices[0, i].item()\n",
    "            prob = top_probs[0, i].item()\n",
    "            conf = confidence[0, 0].item()\n",
    "            tool_name = id_to_tool.get(tool_id, \"<UNK>\")\n",
    "            predictions.append((tool_name, prob, conf))\n",
    "        \n",
    "        return predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0b7c001b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ==== Enhanced Data Generation ====\n",
    "def generate_realistic_sequences(tool_vocab, tool_patterns, num_sequences=200, min_len=3, max_len=8):\n",
    "    \"\"\"Generate more realistic tool usage sequences\"\"\"\n",
    "    sequences = []\n",
    "    \n",
    "    # Pattern-based generation\n",
    "    for _ in range(num_sequences // 2):\n",
    "        seq = []\n",
    "        current_tool = random.choice(tool_vocab)\n",
    "        seq.append(current_tool)\n",
    "        \n",
    "        length = random.randint(min_len, max_len)\n",
    "        for _ in range(length - 1):\n",
    "            if current_tool in tool_patterns:\n",
    "                # 70% chance to follow pattern, 30% random\n",
    "                if random.random() < 0.7:\n",
    "                    next_tool = random.choice(tool_patterns[current_tool])\n",
    "                else:\n",
    "                    next_tool = random.choice(tool_vocab)\n",
    "            else:\n",
    "                next_tool = random.choice(tool_vocab)\n",
    "            seq.append(next_tool)\n",
    "            current_tool = next_tool\n",
    "        \n",
    "        sequences.append(seq)\n",
    "    \n",
    "    # Pure random generation\n",
    "    for _ in range(num_sequences // 2):\n",
    "        seq = random.choices(tool_vocab, k=random.randint(min_len, max_len))\n",
    "        sequences.append(seq)\n",
    "    \n",
    "    return sequences"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1176ee15",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ==== Enhanced Sample Data ====\n",
    "user_sequences = common_workflows * 3  # Repeat common patterns\n",
    "user_sequences += generate_realistic_sequences(tool_vocab, tool_patterns, 300)\n",
    "\n",
    "print(f\"Generated {len(user_sequences)} training sequences\")\n",
    "\n",
    "# ==== Run Enhanced Training ====\n",
    "dataset = ToolDataset(user_sequences, context_len, augment_data=True)\n",
    "dataloader = DataLoader(dataset, batch_size=32, shuffle=True)\n",
    "\n",
    "print(f\"Training samples: {len(dataset)}\")\n",
    "\n",
    "model = ToolPredictor(\n",
    "    vocab_size=vocab_size,\n",
    "    embed_dim=128,  # Increased embedding dimension\n",
    "    desc_dim=desc_dim,\n",
    "    n_heads=8,  # More attention heads\n",
    "    num_layers=4,  # More layers\n",
    "    context_len=context_len,\n",
    "    desc_emb_table=desc_id_to_embedding,\n",
    "    dropout=0.1\n",
    ").to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eecd194d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Count parameters\n",
    "total_params = sum(p.numel() for p in model.parameters())\n",
    "print(f\"Model parameters: {total_params:,}\")\n",
    "\n",
    "# Train the model\n",
    "model = train_model(model, dataloader, epochs=100, lr=2e-4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5b398091",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ==== Enhanced Evaluation ====\n",
    "def evaluate_model(model, test_sequences):\n",
    "    model.eval()\n",
    "    correct = 0\n",
    "    total = 0\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        for seq in test_sequences:\n",
    "            if len(seq) < 2:\n",
    "                continue\n",
    "            \n",
    "            for i in range(1, len(seq)):\n",
    "                history = seq[:i]\n",
    "                true_next = seq[i]\n",
    "                \n",
    "                predictions = predict_next_with_confidence(model, history, top_k=1)\n",
    "                predicted_tool = predictions[0][0]\n",
    "                \n",
    "                if predicted_tool == true_next:\n",
    "                    correct += 1\n",
    "                total += 1\n",
    "    \n",
    "    accuracy = correct / total if total > 0 else 0\n",
    "    return accuracy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d981edab",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\vaibh\\anaconda3\\envs\\llm\\Lib\\site-packages\\sentence_transformers\\cross_encoder\\CrossEncoder.py:11: TqdmExperimentalWarning: Using `tqdm.autonotebook.tqdm` in notebook mode. Use `tqdm.tqdm` instead to force console mode (e.g. in jupyter console)\n",
      "  from tqdm.autonotebook import tqdm, trange\n",
      "c:\\Users\\vaibh\\anaconda3\\envs\\llm\\Lib\\site-packages\\threadpoolctl.py:1214: RuntimeWarning: \n",
      "Found Intel OpenMP ('libiomp') and LLVM OpenMP ('libomp') loaded at\n",
      "the same time. Both libraries are known to be incompatible and this\n",
      "can cause random crashes or deadlocks on Linux when loaded in the\n",
      "same Python program.\n",
      "Using threadpoolctl may cause crashes or deadlocks. For more\n",
      "information and possible workarounds, please see\n",
      "    https://github.com/joblib/threadpoolctl/blob/master/multiple_openmp.md\n",
      "\n",
      "  warnings.warn(msg, RuntimeWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using device: cuda\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\vaibh\\anaconda3\\envs\\llm\\Lib\\site-packages\\transformers\\models\\bert\\modeling_bert.py:408: UserWarning: 1Torch was not compiled with flash attention. (Triggered internally at C:\\actions-runner\\_work\\pytorch\\pytorch\\builder\\windows\\pytorch\\aten\\src\\ATen\\native\\transformers\\cuda\\sdp_utils.cpp:555.)\n",
      "  attn_output = torch.nn.functional.scaled_dot_product_attention(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generated 390 training sequences\n",
      "Training samples: 1947\n",
      "Model parameters: 2,768,185\n",
      "Epoch 1/100: New best model found with loss: 3.5436. Saving checkpoint...\n",
      "Epoch 1/100, Loss: 3.5436, LR: 0.000200\n",
      "Epoch 2/100: New best model found with loss: 3.2496. Saving checkpoint...\n",
      "Epoch 3/100: New best model found with loss: 2.9594. Saving checkpoint...\n",
      "Epoch 4/100: New best model found with loss: 2.6776. Saving checkpoint...\n",
      "Epoch 5/100: New best model found with loss: 2.5002. Saving checkpoint...\n",
      "Epoch 6/100: New best model found with loss: 2.3041. Saving checkpoint...\n",
      "Epoch 6/100, Loss: 2.3041, LR: 0.000200\n",
      "Epoch 7/100: New best model found with loss: 2.1845. Saving checkpoint...\n",
      "Epoch 8/100: New best model found with loss: 2.0165. Saving checkpoint...\n",
      "Epoch 9/100: New best model found with loss: 1.9151. Saving checkpoint...\n",
      "Epoch 10/100: New best model found with loss: 1.7832. Saving checkpoint...\n",
      "Epoch 11/100: New best model found with loss: 1.6676. Saving checkpoint...\n",
      "Epoch 11/100, Loss: 1.6676, LR: 0.000200\n",
      "Epoch 12/100: New best model found with loss: 1.5584. Saving checkpoint...\n",
      "Epoch 13/100: New best model found with loss: 1.4727. Saving checkpoint...\n",
      "Epoch 14/100: New best model found with loss: 1.3723. Saving checkpoint...\n",
      "Epoch 15/100: New best model found with loss: 1.3064. Saving checkpoint...\n",
      "Epoch 16/100: New best model found with loss: 1.2195. Saving checkpoint...\n",
      "Epoch 16/100, Loss: 1.2195, LR: 0.000200\n",
      "Epoch 17/100: New best model found with loss: 1.1376. Saving checkpoint...\n",
      "Epoch 18/100: New best model found with loss: 1.0698. Saving checkpoint...\n",
      "Epoch 19/100: New best model found with loss: 1.0616. Saving checkpoint...\n",
      "Epoch 20/100: New best model found with loss: 1.0034. Saving checkpoint...\n",
      "Epoch 21/100: New best model found with loss: 0.9367. Saving checkpoint...\n",
      "Epoch 21/100, Loss: 0.9367, LR: 0.000200\n",
      "Epoch 22/100: New best model found with loss: 0.9084. Saving checkpoint...\n",
      "Epoch 23/100: New best model found with loss: 0.8792. Saving checkpoint...\n",
      "Epoch 24/100: New best model found with loss: 0.8630. Saving checkpoint...\n",
      "Epoch 25/100: New best model found with loss: 0.8511. Saving checkpoint...\n",
      "Epoch 26/100: New best model found with loss: 0.8150. Saving checkpoint...\n",
      "Epoch 26/100, Loss: 0.8150, LR: 0.000200\n",
      "Epoch 28/100: New best model found with loss: 0.7930. Saving checkpoint...\n",
      "Epoch 29/100: New best model found with loss: 0.7827. Saving checkpoint...\n",
      "Epoch 30/100: New best model found with loss: 0.7733. Saving checkpoint...\n",
      "Epoch 31/100: New best model found with loss: 0.7355. Saving checkpoint...\n",
      "Epoch 31/100, Loss: 0.7355, LR: 0.000200\n",
      "Epoch 33/100: New best model found with loss: 0.7088. Saving checkpoint...\n",
      "Epoch 36/100: New best model found with loss: 0.7032. Saving checkpoint...\n",
      "Epoch 36/100, Loss: 0.7032, LR: 0.000200\n",
      "Epoch 37/100: New best model found with loss: 0.6877. Saving checkpoint...\n",
      "Epoch 38/100: New best model found with loss: 0.6818. Saving checkpoint...\n",
      "Epoch 40/100: New best model found with loss: 0.6775. Saving checkpoint...\n",
      "Epoch 41/100: New best model found with loss: 0.6744. Saving checkpoint...\n",
      "Epoch 41/100, Loss: 0.6744, LR: 0.000200\n",
      "Epoch 42/100: New best model found with loss: 0.6524. Saving checkpoint...\n",
      "Epoch 45/100: New best model found with loss: 0.6416. Saving checkpoint...\n",
      "Epoch 46/100: New best model found with loss: 0.6391. Saving checkpoint...\n",
      "Epoch 46/100, Loss: 0.6391, LR: 0.000200\n",
      "Epoch 48/100: New best model found with loss: 0.6390. Saving checkpoint...\n",
      "Epoch 51/100: New best model found with loss: 0.6341. Saving checkpoint...\n",
      "Epoch 51/100, Loss: 0.6341, LR: 0.000200\n",
      "Epoch 52/100: New best model found with loss: 0.6162. Saving checkpoint...\n",
      "Epoch 55/100: New best model found with loss: 0.6096. Saving checkpoint...\n",
      "Epoch 56/100, Loss: 0.6171, LR: 0.000200\n",
      "Epoch 57/100: New best model found with loss: 0.6076. Saving checkpoint...\n",
      "Epoch 58/100: New best model found with loss: 0.6060. Saving checkpoint...\n",
      "Epoch 59/100: New best model found with loss: 0.6038. Saving checkpoint...\n",
      "Epoch 60/100: New best model found with loss: 0.6024. Saving checkpoint...\n",
      "Epoch 61/100: New best model found with loss: 0.5961. Saving checkpoint...\n",
      "Epoch 61/100, Loss: 0.5961, LR: 0.000200\n",
      "Epoch 63/100: New best model found with loss: 0.5890. Saving checkpoint...\n",
      "Epoch 64/100: New best model found with loss: 0.5867. Saving checkpoint...\n",
      "Epoch 65/100: New best model found with loss: 0.5832. Saving checkpoint...\n",
      "Epoch 66/100, Loss: 0.6056, LR: 0.000200\n",
      "Epoch 67/100: New best model found with loss: 0.5781. Saving checkpoint...\n",
      "Epoch 69/100: New best model found with loss: 0.5681. Saving checkpoint...\n",
      "Epoch 71/100, Loss: 0.5746, LR: 0.000200\n",
      "Epoch 75/100: New best model found with loss: 0.5650. Saving checkpoint...\n",
      "Epoch 76/100: New best model found with loss: 0.5554. Saving checkpoint...\n",
      "Epoch 76/100, Loss: 0.5554, LR: 0.000200\n",
      "Epoch 77/100: New best model found with loss: 0.5549. Saving checkpoint...\n",
      "Epoch 81/100, Loss: 0.5719, LR: 0.000200\n",
      "Epoch 82/100: New best model found with loss: 0.5533. Saving checkpoint...\n",
      "Epoch 85/100: New best model found with loss: 0.5485. Saving checkpoint...\n",
      "Epoch 86/100: New best model found with loss: 0.5449. Saving checkpoint...\n",
      "Epoch 86/100, Loss: 0.5449, LR: 0.000200\n",
      "Epoch 88/100: New best model found with loss: 0.5446. Saving checkpoint...\n",
      "Epoch 90/100: New best model found with loss: 0.5394. Saving checkpoint...\n",
      "Epoch 91/100, Loss: 0.5527, LR: 0.000200\n",
      "Epoch 95/100: New best model found with loss: 0.5271. Saving checkpoint...\n",
      "Epoch 96/100, Loss: 0.5410, LR: 0.000200\n",
      "Test Accuracy: 0.071\n",
      "\n",
      "=== Prediction Examples ===\n",
      "History: ['select', 'copy']\n",
      "  1. paste (prob: 0.987, conf: 0.986)\n",
      "  2. indent (prob: 0.002, conf: 0.986)\n",
      "  3. insert_shape (prob: 0.002, conf: 0.986)\n",
      "\n",
      "History: ['cut', 'paste']\n",
      "  1. bold (prob: 0.959, conf: 0.967)\n",
      "  2. cut (prob: 0.023, conf: 0.967)\n",
      "  3. italic (prob: 0.004, conf: 0.967)\n",
      "\n",
      "History: ['select', 'delete']\n",
      "  1. select (prob: 0.220, conf: 0.995)\n",
      "  2. paste (prob: 0.213, conf: 0.995)\n",
      "  3. web_layout (prob: 0.156, conf: 0.995)\n",
      "\n",
      "History: ['undo', 'undo']\n",
      "  1. cut (prob: 0.269, conf: 0.636)\n",
      "  2. redo (prob: 0.213, conf: 0.636)\n",
      "  3. delete (prob: 0.092, conf: 0.636)\n",
      "\n",
      "History: ['select', 'bold']\n",
      "  1. italic (prob: 0.970, conf: 0.961)\n",
      "  2. underline (prob: 0.007, conf: 0.961)\n",
      "  3. zoom_out (prob: 0.005, conf: 0.961)\n",
      "\n",
      "History: ['select', 'italic', 'underline']\n",
      "  1. font_size (prob: 0.520, conf: 0.987)\n",
      "  2. save (prob: 0.130, conf: 0.987)\n",
      "  3. insert_table (prob: 0.069, conf: 0.987)\n",
      "\n",
      "History: ['align_center']\n",
      "  1. align_left (prob: 0.278, conf: 0.124)\n",
      "  2. grammar_check (prob: 0.123, conf: 0.124)\n",
      "  3. select (prob: 0.106, conf: 0.124)\n",
      "\n",
      "History: ['font_size', 'font_family']\n",
      "  1. track_changes (prob: 0.411, conf: 0.969)\n",
      "  2. page_layout (prob: 0.156, conf: 0.969)\n",
      "  3. insert_header (prob: 0.109, conf: 0.969)\n",
      "\n",
      "History: ['new_document', 'save_as']\n",
      "  1. select (prob: 0.931, conf: 0.967)\n",
      "  2. print (prob: 0.025, conf: 0.967)\n",
      "  3. insert_image (prob: 0.013, conf: 0.967)\n",
      "\n",
      "History: ['open']\n",
      "  1. export_pdf (prob: 0.227, conf: 0.095)\n",
      "  2. find (prob: 0.147, conf: 0.095)\n",
      "  3. new_document (prob: 0.138, conf: 0.095)\n",
      "\n",
      "History: ['save', 'print']\n",
      "  1. page_layout (prob: 0.304, conf: 0.886)\n",
      "  2. new_document (prob: 0.207, conf: 0.886)\n",
      "  3. save_as (prob: 0.186, conf: 0.886)\n",
      "\n",
      "History: ['find', 'replace']\n",
      "  1. save (prob: 0.988, conf: 0.950)\n",
      "  2. insert_image (prob: 0.003, conf: 0.950)\n",
      "  3. print (prob: 0.002, conf: 0.950)\n",
      "\n",
      "History: ['insert_image', 'select']\n",
      "  1. zoom_out (prob: 0.997, conf: 0.984)\n",
      "  2. toggle_ruler (prob: 0.001, conf: 0.984)\n",
      "  3. cut (prob: 0.000, conf: 0.984)\n",
      "\n",
      "History: ['insert_table', 'insert_link']\n",
      "  1. select (prob: 0.870, conf: 0.942)\n",
      "  2. copy (prob: 0.051, conf: 0.942)\n",
      "  3. find (prob: 0.017, conf: 0.942)\n",
      "\n",
      "History: ['bullet_list', 'indent']\n",
      "  1. bullet_list (prob: 0.998, conf: 0.983)\n",
      "  2. outdent (prob: 0.001, conf: 0.983)\n",
      "  3. numbered_list (prob: 0.000, conf: 0.983)\n",
      "\n",
      "History: ['numbered_list', 'outdent']\n",
      "  1. grammar_check (prob: 0.983, conf: 0.991)\n",
      "  2. insert_table (prob: 0.004, conf: 0.991)\n",
      "  3. save (prob: 0.002, conf: 0.991)\n",
      "\n",
      "History: ['share_document', 'add_comment']\n",
      "  1. strike_through (prob: 0.727, conf: 0.318)\n",
      "  2. font_family (prob: 0.217, conf: 0.318)\n",
      "  3. share_document (prob: 0.012, conf: 0.318)\n",
      "\n",
      "History: ['track_changes', 'accept_change']\n",
      "  1. accept_change (prob: 0.990, conf: 0.973)\n",
      "  2. share_document (prob: 0.002, conf: 0.973)\n",
      "  3. reject_change (prob: 0.002, conf: 0.973)\n",
      "\n",
      "History: ['select', 'copy', 'paste', 'undo']\n",
      "  1. delete (prob: 0.520, conf: 0.997)\n",
      "  2. format_painter (prob: 0.184, conf: 0.997)\n",
      "  3. save (prob: 0.103, conf: 0.997)\n",
      "\n",
      "History: ['open', 'find', 'replace', 'save']\n",
      "  1. print (prob: 0.833, conf: 0.992)\n",
      "  2. export_pdf (prob: 0.091, conf: 0.992)\n",
      "  3. new_document (prob: 0.036, conf: 0.992)\n",
      "\n",
      "History: ['new_document', 'insert_header', 'insert_footer']\n",
      "  1. save_as (prob: 0.851, conf: 0.993)\n",
      "  2. insert_image (prob: 0.106, conf: 0.993)\n",
      "  3. save (prob: 0.015, conf: 0.993)\n",
      "\n",
      "History: ['save_as', 'export_pdf', 'share_document']\n",
      "  1. export_pdf (prob: 0.707, conf: 0.872)\n",
      "  2. format_painter (prob: 0.087, conf: 0.872)\n",
      "  3. insert_header (prob: 0.059, conf: 0.872)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Generate test sequences\n",
    "test_sequences = generate_realistic_sequences(tool_vocab, tool_patterns, 50, min_len=3, max_len=6)\n",
    "test_accuracy = evaluate_model(model, test_sequences)\n",
    "print(f\"Test Accuracy: {test_accuracy:.3f}\")\n",
    "\n",
    "print(\"\\n=== Prediction Examples ===\")\n",
    "for history in test_histories:\n",
    "    predictions = predict_next_with_confidence(model, history, top_k=3)\n",
    "    print(f\"History: {history}\")\n",
    "    for i, (tool, prob, conf) in enumerate(predictions):\n",
    "        print(f\"  {i+1}. {tool} (prob: {prob:.3f}, conf: {conf:.3f})\")\n",
    "    print()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3de14fdd",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "llm",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
